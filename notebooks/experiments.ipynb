{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:12:30.410777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-26 09:12:30.515096: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-26 09:12:30.913793: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sudha/cuda/lib64:/usr/local/cuda-11.2/lib64:/home/sudha/miniconda3/envs/tf/lib/\n",
      "2022-11-26 09:12:30.913843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sudha/cuda/lib64:/usr/local/cuda-11.2/lib64:/home/sudha/miniconda3/envs/tf/lib/\n",
      "2022-11-26 09:12:30.913848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:12:31.355029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.355224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.360053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.360246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.360371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.360530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.361057: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-26 09:12:31.487754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.487909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.488030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.488140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.488248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:31.488356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:32.073746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:32.073922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:32.074044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:32.074161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:32.074254: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-11-26 09:12:32.074334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-11-26 09:12:32.074577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:32.074696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-26 09:12:32.074780: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 1\n",
      "2022-11-26 09:12:32.074806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 4096 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "# tf.data.experimental.enable_debug_mode()\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    # tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    # tf.config.set_visible_devices(gpu, 'GPU')\n",
    "    tf.config.set_logical_device_configuration(gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(\n",
    "#     str(i) for i in range(len(tf.config.experimental.list_logical_devices('GPU'))))\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(tf.config.list_logical_devices('GPU'),\n",
    "                                          cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "keras = tf.keras\n",
    "from keras.utils import array_to_img, img_to_array, load_img\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Batch Size: 64\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "BASE_PATH = \"../src/data\"\n",
    "DATA_PATH = f\"{BASE_PATH}/external\"\n",
    "STATE_PATH = f\"{BASE_PATH}/state\"\n",
    "IMAGE_PATH = f'{DATA_PATH}/ISIC_2019_Training_Input'\n",
    "IMAGE_TYPE = 'jpg'\n",
    "MODELS_PATH = f\"{BASE_PATH}/models\"\n",
    "TARGET_SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "CONTRAST_FACTOR = 3  # 2 is performing better and so choosing that instead\n",
    "# CONTRAST_FACTOR = 2\n",
    "DELTA = 0.3\n",
    "IS_FLIP_ON = True\n",
    "RANDOM_STATE = 42\n",
    "# KERNEL_SIZE = (5, 5)  # decreasing seeing 3,3 doing better\n",
    "KERNEL_SIZE = (3, 3)\n",
    "# LEARNING_RATE = 0.001  # decreasing seeing too much ripple effect\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 32\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "print(f\"Global Batch Size: {GLOBAL_BATCH_SIZE}\")\n",
    "TEST_SIZE = 0.2\n",
    "# EPOCHS = 5  # validation accuracy seems to increase with the new learning rate and so increasing the number of epochs\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def reset_random_seeds(seed=RANDOM_STATE):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def clear_session():\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_metadata = pd.read_csv(f'{DATA_PATH}/ISIC_2019_Training_Metadata.csv')\n",
    "df_truth = pd.read_csv(f'{DATA_PATH}/ISIC_2019_Training_GroundTruth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK'], dtype='object')\n",
      "{0: 'MEL', 1: 'NV', 2: 'BCC', 3: 'AK', 4: 'BKL', 5: 'DF', 6: 'VASC', 7: 'SCC', 8: 'UNK'}\n"
     ]
    }
   ],
   "source": [
    "labels = df_truth.columns\n",
    "labels = labels[1:]\n",
    "print(labels)\n",
    "label_mapping = {i: label for i, label in enumerate(labels)}\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Labels...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Labels...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dense_labels = df_truth[labels]\n",
    "dense_labels = dense_labels.values\n",
    "print('Dense Labels...')\n",
    "display(dense_labels)\n",
    "truth_labels = np.argmax(dense_labels, axis=-1)\n",
    "print('Truth Labels...')\n",
    "display(truth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>MEL</th>\n",
       "      <th>NV</th>\n",
       "      <th>BCC</th>\n",
       "      <th>AK</th>\n",
       "      <th>BKL</th>\n",
       "      <th>DF</th>\n",
       "      <th>VASC</th>\n",
       "      <th>SCC</th>\n",
       "      <th>UNK</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0000002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25326</th>\n",
       "      <td>ISIC_0073247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25327</th>\n",
       "      <td>ISIC_0073248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>ISIC_0073249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25329</th>\n",
       "      <td>ISIC_0073251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25330</th>\n",
       "      <td>ISIC_0073254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25331 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image  MEL   NV  BCC   AK  BKL   DF  VASC  SCC  UNK  label\n",
       "0      ISIC_0000000  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1\n",
       "1      ISIC_0000001  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1\n",
       "2      ISIC_0000002  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0\n",
       "3      ISIC_0000003  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1\n",
       "4      ISIC_0000004  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0\n",
       "...             ...  ...  ...  ...  ...  ...  ...   ...  ...  ...    ...\n",
       "25326  ISIC_0073247  0.0  0.0  1.0  0.0  0.0  0.0   0.0  0.0  0.0      2\n",
       "25327  ISIC_0073248  0.0  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0      4\n",
       "25328  ISIC_0073249  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0\n",
       "25329  ISIC_0073251  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1\n",
       "25330  ISIC_0073254  0.0  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0      4\n",
       "\n",
       "[25331 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_truth['label'] = truth_labels.tolist()\n",
    "display(df_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>MEL</th>\n",
       "      <th>NV</th>\n",
       "      <th>BCC</th>\n",
       "      <th>AK</th>\n",
       "      <th>BKL</th>\n",
       "      <th>DF</th>\n",
       "      <th>VASC</th>\n",
       "      <th>SCC</th>\n",
       "      <th>UNK</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0000001</td>\n",
       "      <td>30.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0000002</td>\n",
       "      <td>60.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0000003</td>\n",
       "      <td>30.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0000004</td>\n",
       "      <td>80.0</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25326</th>\n",
       "      <td>ISIC_0073247</td>\n",
       "      <td>85.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>BCN_0003925</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25327</th>\n",
       "      <td>ISIC_0073248</td>\n",
       "      <td>65.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>BCN_0001819</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>ISIC_0073249</td>\n",
       "      <td>70.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>BCN_0001085</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25329</th>\n",
       "      <td>ISIC_0073251</td>\n",
       "      <td>55.0</td>\n",
       "      <td>palms/soles</td>\n",
       "      <td>BCN_0002083</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25330</th>\n",
       "      <td>ISIC_0073254</td>\n",
       "      <td>50.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>BCN_0001079</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25331 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image  age_approx anatom_site_general    lesion_id     sex  MEL  \\\n",
       "0      ISIC_0000000        55.0      anterior torso          NaN  female  0.0   \n",
       "1      ISIC_0000001        30.0      anterior torso          NaN  female  0.0   \n",
       "2      ISIC_0000002        60.0     upper extremity          NaN  female  1.0   \n",
       "3      ISIC_0000003        30.0     upper extremity          NaN    male  0.0   \n",
       "4      ISIC_0000004        80.0     posterior torso          NaN    male  1.0   \n",
       "...             ...         ...                 ...          ...     ...  ...   \n",
       "25326  ISIC_0073247        85.0           head/neck  BCN_0003925  female  0.0   \n",
       "25327  ISIC_0073248        65.0      anterior torso  BCN_0001819    male  0.0   \n",
       "25328  ISIC_0073249        70.0     lower extremity  BCN_0001085    male  1.0   \n",
       "25329  ISIC_0073251        55.0         palms/soles  BCN_0002083  female  0.0   \n",
       "25330  ISIC_0073254        50.0     upper extremity  BCN_0001079    male  0.0   \n",
       "\n",
       "        NV  BCC   AK  BKL   DF  VASC  SCC  UNK  label  \n",
       "0      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1  \n",
       "1      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1  \n",
       "2      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0  \n",
       "3      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1  \n",
       "4      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0  \n",
       "...    ...  ...  ...  ...  ...   ...  ...  ...    ...  \n",
       "25326  0.0  1.0  0.0  0.0  0.0   0.0  0.0  0.0      2  \n",
       "25327  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0      4  \n",
       "25328  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0  \n",
       "25329  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1  \n",
       "25330  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0      4  \n",
       "\n",
       "[25331 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_joined = df_metadata.join(df_truth.set_index('image'), on='image', how='left')\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of null values in anatom_site_general column before null fill: 2631\n",
      "Count of null values in anatom_site_general column after null fill: 0\n"
     ]
    }
   ],
   "source": [
    "# Site details add more information to model and so fill with unknown if not available\n",
    "print(\n",
    "    f\"Count of null values in anatom_site_general column before null fill: {sum(df_joined['anatom_site_general'].isnull())}\")\n",
    "df_joined['anatom_site_general'] = df_joined['anatom_site_general'].fillna('unknown')\n",
    "print(\n",
    "    f\"Count of null values in anatom_site_general column after null fill: {sum(df_joined['anatom_site_general'].isnull())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of null values in sex column before null fill: 384\n",
      "Count of null values in sex column after null fill: 0\n"
     ]
    }
   ],
   "source": [
    "# Sex details add more information to model and so fill with unknown if not available\n",
    "print(f\"Count of null values in sex column before null fill: {sum(df_joined['sex'].isnull())}\")\n",
    "df_joined['sex'] = df_joined['sex'].fillna('unknown')\n",
    "print(f\"Count of null values in sex column after null fill: {sum(df_joined['sex'].isnull())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in cleaned dataframe before na/inf/zero remove: 24894\n",
      "Observations in cleaned dataframe after na/inf/zero remove: 24840\n"
     ]
    }
   ],
   "source": [
    "# Age is important for the model and so we remove rows with na/inf/zero values\n",
    "df_cleaned = df_joined[~df_joined['age_approx'].isnull()]\n",
    "print(f\"Observations in cleaned dataframe before na/inf/zero remove: {len(df_cleaned)}\")\n",
    "df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_cleaned.dropna(how=\"all\", inplace=True)\n",
    "df_cleaned = df_cleaned[df_cleaned['age_approx'] > 0]\n",
    "print(f\"Observations in cleaned dataframe after na/inf/zero remove: {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert age column to uint8 for resampling\n",
    "# df_cleaned['age_approx'] = df_cleaned['age_approx'].astype(np.uint8)\n",
    "# display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>MEL</th>\n",
       "      <th>NV</th>\n",
       "      <th>BCC</th>\n",
       "      <th>AK</th>\n",
       "      <th>BKL</th>\n",
       "      <th>DF</th>\n",
       "      <th>VASC</th>\n",
       "      <th>SCC</th>\n",
       "      <th>UNK</th>\n",
       "      <th>label</th>\n",
       "      <th>anatomy_site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>torso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0000001</td>\n",
       "      <td>30.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>torso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0000002</td>\n",
       "      <td>60.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>extremity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0000003</td>\n",
       "      <td>30.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>extremity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0000004</td>\n",
       "      <td>80.0</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>torso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25326</th>\n",
       "      <td>ISIC_0073247</td>\n",
       "      <td>85.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>BCN_0003925</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>head/neck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25327</th>\n",
       "      <td>ISIC_0073248</td>\n",
       "      <td>65.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>BCN_0001819</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>torso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>ISIC_0073249</td>\n",
       "      <td>70.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>BCN_0001085</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>extremity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25329</th>\n",
       "      <td>ISIC_0073251</td>\n",
       "      <td>55.0</td>\n",
       "      <td>palms/soles</td>\n",
       "      <td>BCN_0002083</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>palms/soles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25330</th>\n",
       "      <td>ISIC_0073254</td>\n",
       "      <td>50.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>BCN_0001079</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>extremity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24840 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image  age_approx anatom_site_general    lesion_id     sex  MEL  \\\n",
       "0      ISIC_0000000        55.0      anterior torso          NaN  female  0.0   \n",
       "1      ISIC_0000001        30.0      anterior torso          NaN  female  0.0   \n",
       "2      ISIC_0000002        60.0     upper extremity          NaN  female  1.0   \n",
       "3      ISIC_0000003        30.0     upper extremity          NaN    male  0.0   \n",
       "4      ISIC_0000004        80.0     posterior torso          NaN    male  1.0   \n",
       "...             ...         ...                 ...          ...     ...  ...   \n",
       "25326  ISIC_0073247        85.0           head/neck  BCN_0003925  female  0.0   \n",
       "25327  ISIC_0073248        65.0      anterior torso  BCN_0001819    male  0.0   \n",
       "25328  ISIC_0073249        70.0     lower extremity  BCN_0001085    male  1.0   \n",
       "25329  ISIC_0073251        55.0         palms/soles  BCN_0002083  female  0.0   \n",
       "25330  ISIC_0073254        50.0     upper extremity  BCN_0001079    male  0.0   \n",
       "\n",
       "        NV  BCC   AK  BKL   DF  VASC  SCC  UNK  label anatomy_site  \n",
       "0      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1        torso  \n",
       "1      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1        torso  \n",
       "2      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0    extremity  \n",
       "3      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1    extremity  \n",
       "4      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0        torso  \n",
       "...    ...  ...  ...  ...  ...   ...  ...  ...    ...          ...  \n",
       "25326  0.0  1.0  0.0  0.0  0.0   0.0  0.0  0.0      2    head/neck  \n",
       "25327  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0      4        torso  \n",
       "25328  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0    extremity  \n",
       "25329  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1  palms/soles  \n",
       "25330  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0      4    extremity  \n",
       "\n",
       "[24840 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# decreasing the scope of anatomy site can provide more information and so getting a more generic information\n",
    "df_cleaned['anatomy_site'] = df_cleaned['anatom_site_general'].apply(lambda anatomy: anatomy.split()[-1])\n",
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>MEL</th>\n",
       "      <th>NV</th>\n",
       "      <th>BCC</th>\n",
       "      <th>AK</th>\n",
       "      <th>BKL</th>\n",
       "      <th>DF</th>\n",
       "      <th>VASC</th>\n",
       "      <th>SCC</th>\n",
       "      <th>UNK</th>\n",
       "      <th>label</th>\n",
       "      <th>anatomy_site</th>\n",
       "      <th>is_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>ISIC_0024318</td>\n",
       "      <td>65.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>HAM_0002450</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>extremity</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>ISIC_0024370</td>\n",
       "      <td>55.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>HAM_0001780</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>unknown</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>ISIC_0024386</td>\n",
       "      <td>40.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>HAM_0005112</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>extremity</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>ISIC_0024475</td>\n",
       "      <td>35.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>HAM_0003873</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>ISIC_0024517</td>\n",
       "      <td>65.0</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>HAM_0001894</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>torso</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25256</th>\n",
       "      <td>ISIC_0073141</td>\n",
       "      <td>45.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>BCN_0005520</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>torso</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25259</th>\n",
       "      <td>ISIC_0073144</td>\n",
       "      <td>75.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>BCN_0004091</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>torso</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25287</th>\n",
       "      <td>ISIC_0073193</td>\n",
       "      <td>35.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>BCN_0002147</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>torso</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25289</th>\n",
       "      <td>ISIC_0073195</td>\n",
       "      <td>70.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>BCN_0005492</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>extremity</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25310</th>\n",
       "      <td>ISIC_0073223</td>\n",
       "      <td>80.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>BCN_0005542</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>extremity</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1520 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image  age_approx anatom_site_general    lesion_id     sex  MEL  \\\n",
       "2915   ISIC_0024318        65.0     lower extremity  HAM_0002450  female  0.0   \n",
       "2967   ISIC_0024370        55.0             unknown  HAM_0001780    male  0.0   \n",
       "2983   ISIC_0024386        40.0     lower extremity  HAM_0005112  female  0.0   \n",
       "3072   ISIC_0024475        35.0           head/neck  HAM_0003873    male  0.0   \n",
       "3114   ISIC_0024517        65.0     posterior torso  HAM_0001894    male  0.0   \n",
       "...             ...         ...                 ...          ...     ...  ...   \n",
       "25256  ISIC_0073141        45.0      anterior torso  BCN_0005520  female  0.0   \n",
       "25259  ISIC_0073144        75.0      anterior torso  BCN_0004091  female  0.0   \n",
       "25287  ISIC_0073193        35.0      anterior torso  BCN_0002147    male  0.0   \n",
       "25289  ISIC_0073195        70.0     lower extremity  BCN_0005492    male  0.0   \n",
       "25310  ISIC_0073223        80.0     upper extremity  BCN_0005542  female  0.0   \n",
       "\n",
       "        NV  BCC   AK  BKL   DF  VASC  SCC  UNK  label anatomy_site  is_seen  \n",
       "2915   0.0  0.0  0.0  0.0  1.0   0.0  0.0  0.0      5    extremity     True  \n",
       "2967   0.0  0.0  0.0  0.0  0.0   1.0  0.0  0.0      6      unknown     True  \n",
       "2983   0.0  0.0  0.0  0.0  1.0   0.0  0.0  0.0      5    extremity     True  \n",
       "3072   0.0  0.0  0.0  0.0  0.0   1.0  0.0  0.0      6    head/neck     True  \n",
       "3114   0.0  0.0  0.0  0.0  0.0   0.0  1.0  0.0      7        torso     True  \n",
       "...    ...  ...  ...  ...  ...   ...  ...  ...    ...          ...      ...  \n",
       "25256  0.0  0.0  0.0  0.0  1.0   0.0  0.0  0.0      5        torso     True  \n",
       "25259  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1        torso     True  \n",
       "25287  0.0  0.0  0.0  0.0  1.0   0.0  0.0  0.0      5        torso     True  \n",
       "25289  0.0  0.0  0.0  0.0  0.0   0.0  1.0  0.0      7    extremity     True  \n",
       "25310  0.0  0.0  0.0  0.0  0.0   0.0  1.0  0.0      7    extremity     True  \n",
       "\n",
       "[1520 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>MEL</th>\n",
       "      <th>NV</th>\n",
       "      <th>BCC</th>\n",
       "      <th>AK</th>\n",
       "      <th>BKL</th>\n",
       "      <th>DF</th>\n",
       "      <th>VASC</th>\n",
       "      <th>SCC</th>\n",
       "      <th>UNK</th>\n",
       "      <th>label</th>\n",
       "      <th>anatomy_site</th>\n",
       "      <th>is_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>torso</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0000001</td>\n",
       "      <td>30.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>torso</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0000002</td>\n",
       "      <td>60.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>extremity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0000003</td>\n",
       "      <td>30.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>extremity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0000004</td>\n",
       "      <td>80.0</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>torso</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25326</th>\n",
       "      <td>ISIC_0073247</td>\n",
       "      <td>85.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>BCN_0003925</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25327</th>\n",
       "      <td>ISIC_0073248</td>\n",
       "      <td>65.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>BCN_0001819</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>torso</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>ISIC_0073249</td>\n",
       "      <td>70.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>BCN_0001085</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>extremity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25329</th>\n",
       "      <td>ISIC_0073251</td>\n",
       "      <td>55.0</td>\n",
       "      <td>palms/soles</td>\n",
       "      <td>BCN_0002083</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>palms/soles</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25330</th>\n",
       "      <td>ISIC_0073254</td>\n",
       "      <td>50.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>BCN_0001079</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>extremity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23320 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image  age_approx anatom_site_general    lesion_id     sex  MEL  \\\n",
       "0      ISIC_0000000        55.0      anterior torso          NaN  female  0.0   \n",
       "1      ISIC_0000001        30.0      anterior torso          NaN  female  0.0   \n",
       "2      ISIC_0000002        60.0     upper extremity          NaN  female  1.0   \n",
       "3      ISIC_0000003        30.0     upper extremity          NaN    male  0.0   \n",
       "4      ISIC_0000004        80.0     posterior torso          NaN    male  1.0   \n",
       "...             ...         ...                 ...          ...     ...  ...   \n",
       "25326  ISIC_0073247        85.0           head/neck  BCN_0003925  female  0.0   \n",
       "25327  ISIC_0073248        65.0      anterior torso  BCN_0001819    male  0.0   \n",
       "25328  ISIC_0073249        70.0     lower extremity  BCN_0001085    male  1.0   \n",
       "25329  ISIC_0073251        55.0         palms/soles  BCN_0002083  female  0.0   \n",
       "25330  ISIC_0073254        50.0     upper extremity  BCN_0001079    male  0.0   \n",
       "\n",
       "        NV  BCC   AK  BKL   DF  VASC  SCC  UNK  label anatomy_site  is_seen  \n",
       "0      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1        torso    False  \n",
       "1      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1        torso    False  \n",
       "2      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0    extremity    False  \n",
       "3      1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1    extremity    False  \n",
       "4      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0        torso    False  \n",
       "...    ...  ...  ...  ...  ...   ...  ...  ...    ...          ...      ...  \n",
       "25326  0.0  1.0  0.0  0.0  0.0   0.0  0.0  0.0      2    head/neck    False  \n",
       "25327  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0      4        torso    False  \n",
       "25328  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      0    extremity    False  \n",
       "25329  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0      1  palms/soles    False  \n",
       "25330  0.0  0.0  0.0  1.0  0.0   0.0  0.0  0.0      4    extremity    False  \n",
       "\n",
       "[23320 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f\"{STATE_PATH}/images_seen.txt\") as seen_file:\n",
    "    images_seen = seen_file.read()\n",
    "    images_seen = images_seen.split('\\n')\n",
    "    images_seen = set(images_seen)\n",
    "df_cleaned['is_seen'] = df_cleaned['image'].apply(lambda name: name in images_seen)\n",
    "\n",
    "df_seen = df_cleaned[df_cleaned['is_seen']].copy()\n",
    "display(df_seen)\n",
    "df_test = df_cleaned[df_cleaned['is_seen'] == False].copy()\n",
    "# Limit the number of test results to speed up experiments\n",
    "# df_test = df_test.head(df_seen.shape[0])\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Predicting everything as MEL (label = 0) since that is the most dangerous type of skin lesion!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 18.21%\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = (df_test['label'] == 0).sum() / df_test.shape[0]\n",
    "print(f\"Accuracy on test data: {accuracy_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_img_data(img_names):\n",
    "    images = []\n",
    "    for img_name in img_names:\n",
    "        img = load_img(f\"{IMAGE_PATH}/{img_name}.{IMAGE_TYPE}\", target_size=TARGET_SIZE)\n",
    "        img = img_to_array(img)\n",
    "        images.append(img)\n",
    "    return np.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def aug_img_data(images, size=TARGET_SIZE, delta=DELTA, contrast_factor=CONTRAST_FACTOR, is_flip_on=IS_FLIP_ON,\n",
    "                 **kwargs):\n",
    "    # make a copy of images\n",
    "    images_aug = copy.deepcopy(images)\n",
    "    # image resize\n",
    "    images_aug = tf.image.resize(images_aug, size=size)\n",
    "\n",
    "    # brightness\n",
    "    images_aug = tf.image.adjust_brightness(images_aug, delta=delta)\n",
    "    # contrast\n",
    "    images_aug = tf.image.adjust_contrast(images_aug, contrast_factor=contrast_factor)\n",
    "    # random flip\n",
    "    if is_flip_on:\n",
    "        images_aug = tf.image.random_flip_left_right(images_aug)\n",
    "\n",
    "    return images_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 102\n",
      "Length of validation data: 26\n",
      "Length of test data: 256\n"
     ]
    }
   ],
   "source": [
    "LIMIT = GLOBAL_BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "df_seen = df_seen.iloc[:LIMIT]\n",
    "df_test = df_test.iloc[:LIMIT * strategy.num_replicas_in_sync]\n",
    "\n",
    "img_seen = load_img_data(df_seen['image'])\n",
    "y_seen = df_seen['label']\n",
    "\n",
    "img_train, img_val, y_train, y_val = train_test_split(img_seen, y_seen, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "print(f\"Length of training data: {img_train.shape[0]}\")\n",
    "print(f\"Length of validation data: {img_val.shape[0]}\")\n",
    "\n",
    "img_test = load_img_data(df_test['image'])\n",
    "y_test = df_test['label']\n",
    "print(f\"Length of test data: {img_test.shape[0]}\")\n",
    "\n",
    "image_index = {name: i for i, name in enumerate(df_cleaned['image'])}\n",
    "rev_image_index = {i: name for i, name in enumerate(df_cleaned['image'])}\n",
    "df_seen['image'] = df_seen['image'].apply(lambda name: image_index[name])\n",
    "df_test['image'] = df_test['image'].apply(lambda name: image_index[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    x_arr = np.arange(len(hist['loss'])) + 1\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
    "    ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Loss', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n",
    "    ax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation acc.')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Accuracy', size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(kernel_size=KERNEL_SIZE, strides=(1, 1), pool_size=(2, 2), optimizer='Adam',\n",
    "                    learning_rate=LEARNING_RATE, conv_filters=[32, 64], hidden_units=[1024, 256], dropout_rate=0.8,\n",
    "                    **kwargs):\n",
    "    # def build_cnn_model(kernel_size=(5, 5), strides=(1, 1), pool_size=(2, 2), optimizer='Adam', learning_rate=0.001, conv_filters=[32, 64], hidden_units=[1024], dropout_rate=0.5, **kwargs):\n",
    "    clear_session()\n",
    "    reset_random_seeds()\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    for i, filters in enumerate(conv_filters):\n",
    "        i += 1\n",
    "        model.add(keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same',\n",
    "                                      name=f'conv_{i}', activation='relu'))\n",
    "        model.add(keras.layers.MaxPool2D(pool_size=pool_size, name=f'pool_{i}'))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    for i, units in enumerate(hidden_units):\n",
    "        i += 1\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "        model.add(keras.layers.Dense(units=units, name=f'fc_{i}', activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(keras.layers.Dense(units=len(labels), name='output', activation=\"softmax\"))\n",
    "    model.build(input_shape=(None, *TARGET_SIZE, 3))\n",
    "\n",
    "    optimizer_mapping = {\n",
    "        'sgd': keras.optimizers.SGD,\n",
    "        'adam': keras.optimizers.Adam,\n",
    "    }\n",
    "    optimizer = optimizer_mapping[optimizer.lower()]\n",
    "    optimizer = optimizer(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augment train data\n",
    "img_seen_aug = aug_img_data(img_seen)\n",
    "img_seen_aug = tf.concat([img_seen, img_seen_aug], axis=0)\n",
    "y_seen_aug = tf.concat([y_seen, y_seen], axis=0)\n",
    "\n",
    "# shuffle the data\n",
    "reset_random_seeds()\n",
    "shuffle = tf.random.shuffle(tf.range(tf.shape(img_seen_aug)[0], dtype=tf.int32))\n",
    "img_seen_aug = tf.gather(img_seen_aug, shuffle)\n",
    "y_seen_aug = tf.gather(y_seen_aug, shuffle).numpy()\n",
    "print(f\"Length of augmented seen data: {img_seen_aug.shape[0]}\")\n",
    "\n",
    "if 'model_cnn' in locals() or 'model_cnn' in globals():\n",
    "    del model_cnn\n",
    "model_cnn = build_cnn_model()\n",
    "model_cnn.summary()\n",
    "\n",
    "history_cnn = model_cnn.fit(img_seen_aug, y_seen_aug, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=TEST_SIZE).history\n",
    "plot_history(history_cnn)\n",
    "\n",
    "i = 1100\n",
    "print(f\"embeddings: {build_embedding_df(model_cnn, img_seen_aug[i:i + 1]).to_numpy()}\")\n",
    "print(f\"predicts: {model_cnn.predict(img_seen_aug[i:i+1])}\")\n",
    "print(f\"ground-truth: {y_seen_aug[i:i+1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default_args = {\n",
    "    \"kernel_size\": KERNEL_SIZE,\n",
    "    \"strides\": (1, 1),\n",
    "    \"pool_size\": (2, 2),\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"delta\": DELTA,\n",
    "    \"contrast_factor\": CONTRAST_FACTOR,\n",
    "    \"is_flip_on\": IS_FLIP_ON\n",
    "}\n",
    "new_args = {\n",
    "    # \"kernel_size\": (3, 3),\n",
    "    \"strides\": (2, 2),\n",
    "    # \"pool_size\": (3, 3),\n",
    "    # \"optimizer\": \"SGD\",  # unable to train and so not considering this\n",
    "    # \"learning_rate\": 0.01,  # this is causing ripple effect\n",
    "    \"delta\": 0.1,  # this is performing better and so using as default\n",
    "    \"contrast_factor\": 2,  # this is performing better and so using as default\n",
    "    \"is_flip_on\": False\n",
    "}\n",
    "# history_cnn = history_cnn.history\n",
    "results = [[\n",
    "    history_cnn['accuracy'][-1],\n",
    "    history_cnn['val_accuracy'][-1],\n",
    "    default_args['kernel_size'],\n",
    "    default_args['strides'],\n",
    "    default_args['pool_size'],\n",
    "    default_args['learning_rate'],\n",
    "    default_args['optimizer'],\n",
    "    default_args['delta'],\n",
    "    default_args['contrast_factor'],\n",
    "    default_args['is_flip_on']\n",
    "]]\n",
    "\n",
    "for k, v in new_args.items():\n",
    "    time.sleep(30)\n",
    "    # clear_output(wait=True)\n",
    "    exp_args = copy.deepcopy(default_args)\n",
    "    exp_args[k] = v\n",
    "    print(f\"Experiment: {exp_args}\")\n",
    "    print(f\"Modified: {k} = {v}\")\n",
    "\n",
    "    img_seen_aug = aug_img_data(img_seen, **exp_args)\n",
    "    img_seen_aug = tf.concat([img_seen, img_seen_aug], axis=0)\n",
    "    y_seen_aug = tf.concat([y_seen, y_seen], axis=0)\n",
    "\n",
    "    # shuffle the data\n",
    "    reset_random_seeds()\n",
    "    shuffle = tf.random.shuffle(tf.range(tf.shape(img_seen_aug)[0], dtype=tf.int32))\n",
    "    img_seen_aug = tf.gather(img_seen_aug, shuffle)\n",
    "    y_seen_aug = tf.gather(y_seen_aug, shuffle).numpy()\n",
    "    # print(f\"Length of augmented seen data: {img_seen_aug.shape[0]}\")\n",
    "\n",
    "    if 'model_cnn' in locals() or 'model_cnn' in globals():\n",
    "        del model_cnn\n",
    "    model_cnn = build_cnn_model(**exp_args)\n",
    "    model_cnn.summary()\n",
    "\n",
    "    history_cnn = model_cnn.fit(img_seen_aug, y_seen_aug, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=TEST_SIZE).history\n",
    "    plot_history(history_cnn)\n",
    "\n",
    "    results.append([\n",
    "        history_cnn['accuracy'][-1],\n",
    "        history_cnn['val_accuracy'][-1],\n",
    "        exp_args['kernel_size'],\n",
    "        exp_args['strides'],\n",
    "        exp_args['pool_size'],\n",
    "        exp_args['learning_rate'],\n",
    "        exp_args['optimizer'],\n",
    "        exp_args['delta'],\n",
    "        exp_args['contrast_factor'],\n",
    "        exp_args['is_flip_on']\n",
    "    ])\n",
    "\n",
    "    i = 1100\n",
    "    print(f\"embeddings: {build_embedding_df(model_cnn, img_seen_aug[i:i + 1]).to_numpy()}\")\n",
    "    print(f\"predicts: {model_cnn.predict(img_seen_aug[i:i+1])}\")\n",
    "    print(f\"ground-truth: {y_seen_aug[i:i+1]}\")\n",
    "\n",
    "df_cnn_results = pd.DataFrame(results)\n",
    "display(HTML(df_cnn_results.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Image Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selected CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "CNN_MODEL_FILENAME = f\"{MODELS_PATH}/model_cnn.h5\"\n",
    "if not os.path.isfile(CNN_MODEL_FILENAME):\n",
    "    default_args = {\n",
    "        \"kernel_size\": KERNEL_SIZE,\n",
    "        \"strides\": (1, 1),\n",
    "        \"pool_size\": (2, 2),\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"delta\": DELTA,\n",
    "        \"contrast_factor\": CONTRAST_FACTOR,\n",
    "        \"is_flip_on\": IS_FLIP_ON\n",
    "    }\n",
    "    exp_args = {\n",
    "        **default_args,\n",
    "        \"strides\": (2, 2),\n",
    "        \"contrast_factor\": 2\n",
    "    }\n",
    "\n",
    "    # time.sleep(30)\n",
    "    # clear_output(wait=True)\n",
    "    print(f\"Experiment: {exp_args}\")\n",
    "\n",
    "    img_seen_aug = aug_img_data(img_seen, **exp_args)\n",
    "    img_seen_aug = tf.concat([img_seen, img_seen_aug], axis=0)\n",
    "    y_seen_aug = tf.concat([y_seen, y_seen], axis=0)\n",
    "\n",
    "    # shuffle the data\n",
    "    reset_random_seeds()\n",
    "    shuffle = tf.random.shuffle(tf.range(tf.shape(img_seen_aug)[0], dtype=tf.int32))\n",
    "    img_seen_aug = tf.gather(img_seen_aug, shuffle)\n",
    "    y_seen_aug = tf.gather(y_seen_aug, shuffle).numpy()\n",
    "\n",
    "    if 'model_cnn' in locals() or 'model_cnn' in globals():\n",
    "        del model_cnn\n",
    "    with strategy.scope():\n",
    "        model_cnn = build_cnn_model(**exp_args)\n",
    "    model_cnn.fit(img_seen_aug, y_seen_aug, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=TEST_SIZE)\n",
    "    model_cnn.save(CNN_MODEL_FILENAME)\n",
    "\n",
    "# model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_batch_embedding_df(model_embedding, data_img, index):\n",
    "    # display(data_img)\n",
    "    # display(index)\n",
    "    embedding_img = model_embedding.predict_on_batch(data_img)\n",
    "    embedding_cols = [f\"emb_{i + 1}\" for i in range(embedding_img.shape[1])]\n",
    "    embedding_df = pd.DataFrame(embedding_img, columns=embedding_cols, index=index)\n",
    "    embedding_df.reset_index(inplace=True)\n",
    "    return embedding_df.rename(columns={'index': 'image'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def build_embedding_df(model_embedding, dist_dataset):\n",
    "#     df_embeddings_list = []\n",
    "    \n",
    "#     embedding_img = np.asarray([])\n",
    "#     index = []\n",
    "    \n",
    "#     for dataset, idx in dist_dataset:\n",
    "#         index.extend(idx.values)\n",
    "        \n",
    "#         embeddings = strategy.run(model_embedding.predict_on_batch, args=(dataset,)).values\n",
    "#         embedding_img = np.append(embedding_img, embeddings, axis=0)\n",
    "    \n",
    "#     embedding_cols = [f\"emb_{i + 1}\" for i in range(embedding_img.shape[1])]\n",
    "#     embedding_df = pd.DataFrame(embedding_img, columns=embedding_cols, index=index)\n",
    "#     embedding_df.reset_index(inplace=True)\n",
    "#     return embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # get the flatten, penultimate, and output layers\n",
    "# # layer_outputs = [model.get_layer('flatten'), model.layers[-3], model.layers[-1]]\n",
    "# # layer_outputs = [layer.output for layer in layer_outputs]\n",
    "# # model_img_embedding = keras.models.Model(inputs=model_cnn.input, outputs=layer_outputs)\n",
    "\n",
    "# if 'model_img_embedding' in locals() or 'model_img_embedding' in globals():\n",
    "#     del model_img_embedding\n",
    "# # get penultimate layer\n",
    "# with strategy.scope():\n",
    "#     model_img_embedding = keras.models.Model(inputs=model_cnn.input, outputs=model_cnn.layers[-3].output)\n",
    "\n",
    "# dataset_seen = tf.data.Dataset.from_tensor_slices((img_seen, df_seen['image'])).batch(GLOBAL_BATCH_SIZE) \n",
    "# dataset_test = tf.data.Dataset.from_tensor_slices((img_test, df_test['image'])).batch(GLOBAL_BATCH_SIZE) \n",
    "# dist_dataset_seen = strategy.experimental_distribute_dataset(dataset_seen)\n",
    "# dist_dataset_test = strategy.experimental_distribute_dataset(dataset_test)\n",
    "\n",
    "# df_embeddings_img_seen = build_embedding_df(model_img_embedding, dist_dataset_seen)\n",
    "# display(df_embeddings_img_seen)\n",
    "\n",
    "# df_embeddings_img_test = build_embedding_df(model_img_embedding, dist_dataset_test)\n",
    "# display(df_embeddings_img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert tf.distribute.get_replica_context() is not None  # default\n",
    "# # dataset = tf.data.Dataset.from_tensors((list(range(4)))).batch(4)\n",
    "# print(GLOBAL_BATCH_SIZE)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((np.array(list(range(2 * GLOBAL_BATCH_SIZE))))).batch(GLOBAL_BATCH_SIZE)\n",
    "# dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "                                       \n",
    "# with strategy.scope():\n",
    "#     # assert tf.distribute.get_replica_context() is None\n",
    "\n",
    "#     def f(i):\n",
    "#         print(f\"=========f: {i}\")\n",
    "#         replica_context = tf.distribute.get_replica_context()  # for strategy\n",
    "#         assert replica_context is not None\n",
    "#         tf.print(\"Replica id: \", replica_context.replica_id_in_sync_group + 1,\n",
    "#              \" of \", replica_context.num_replicas_in_sync)\n",
    "#         return 2 * i\n",
    "\n",
    "#     # print(f\"ouside for: {dataset}\")\n",
    "#     for d in dist_dataset:\n",
    "#         # print(f\"inside: {d}\")\n",
    "#         result = strategy.run(f, args=(d,))\n",
    "\n",
    "# tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 09:26:00.378977: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 128\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\025TensorSliceDataset:85\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 224\n",
      "        }\n",
      "        dim {\n",
      "          size: 224\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "index: (64,)\n",
      "dataset-shape: 32\n",
      "dataset-shape: 32\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_667086/1254733102.py\", line 36, in build_embedding_in_batch  *\n        return tf.distribute.get_replica_context().all_gather(model_embedding(dataset), axis=0)\n\n    AttributeError: 'NoneType' object has no attribute 'all_gather'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m dataset_seen \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((img_seen, df_seen[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()))\u001b[38;5;241m.\u001b[39mbatch(GLOBAL_BATCH_SIZE) \n\u001b[1;32m     86\u001b[0m dist_dataset_seen \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(dataset_seen)\n\u001b[0;32m---> 87\u001b[0m df_embeddings_img_seen \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_embedding_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_dataset_seen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m display(df_embeddings_img_seen)\n\u001b[1;32m     90\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors((img_test, df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()))\u001b[38;5;241m.\u001b[39mbatch(GLOBAL_BATCH_SIZE) \n",
      "Cell \u001b[0;32mIn [31], line 59\u001b[0m, in \u001b[0;36mbuild_embedding_df\u001b[0;34m(dist_dataset)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset-shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ix)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# per_replica_embedding = strategy.run(build_embedding_in_batch, args=(dataset,))\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m per_replica_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_embedding_in_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# per_replica_embedding = tf.distribute.get_replica_context().all_gather(model_embedding(dataset), axis=0)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mexperimental_local_results(per_replica_embedding):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filevte4sfby.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__build_embedding_in_batch\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_replica_context, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\u001b[38;5;241m.\u001b[39mall_gather, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model_embedding), (ag__\u001b[38;5;241m.\u001b[39mld(dataset),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), fscope)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_667086/1254733102.py\", line 36, in build_embedding_in_batch  *\n        return tf.distribute.get_replica_context().all_gather(model_embedding(dataset), axis=0)\n\n    AttributeError: 'NoneType' object has no attribute 'all_gather'\n"
     ]
    }
   ],
   "source": [
    "# get the flatten, penultimate, and output layers\n",
    "# layer_outputs = [model.get_layer('flatten'), model.layers[-3], model.layers[-1]]\n",
    "# layer_outputs = [layer.output for layer in layer_outputs]\n",
    "# model_img_embedding = keras.models.Model(inputs=model_cnn.input, outputs=layer_outputs)\n",
    "\n",
    "assert tf.distribute.get_replica_context() is not None  # default\n",
    "\n",
    "# with strategy.scope():\n",
    "    # assert tf.distribute.get_replica_context() is not None  # default\n",
    "    # @tf.function\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "def build_embedding_df(dist_dataset):\n",
    "    # assert tf.distribute.get_replica_context() is None  # default\n",
    "\n",
    "    df_embeddings_list = []\n",
    "\n",
    "    embedding_img = np.asarray([])\n",
    "    index = []\n",
    "    \n",
    "#     class EmbeddingModel(keras.models.Model):\n",
    "#         def __init__(self):\n",
    "#             super(EmbeddingModel, self).__init__()\n",
    "#             model_cnn = keras.models.load_model(CNN_MODEL_FILENAME)\n",
    "#             # get penultimate layer\n",
    "#             self.model = keras.models.Model(inputs=model_cnn.input, outputs=model_cnn.layers[-3].output)\n",
    "            \n",
    "#         def call(self, inputs):\n",
    "#             return self.model.predict(inputs)\n",
    "\n",
    "    @tf.function\n",
    "    def build_embedding_in_batch(dataset):\n",
    "        # print(dataset.shape)\n",
    "        # return tf.distribute.get_replica_context().all_gather(model_embedding.predict(dataset), axis=0)\n",
    "        return tf.distribute.get_replica_context().all_gather(model_embedding(dataset), axis=0)\n",
    "    \n",
    "    @tf.function\n",
    "    def return_const(idx):\n",
    "        return idx\n",
    "\n",
    "    embedding_img = np.empty((0, 256), np.float32)\n",
    "    index = np.empty((0), np.int32)\n",
    "    with strategy.scope():\n",
    "        model_cnn = keras.models.load_model(CNN_MODEL_FILENAME)\n",
    "        model_embedding = keras.models.Model(inputs=model_cnn.input, outputs=model_cnn.layers[-3].output)\n",
    "\n",
    "        for dataset, idx in dist_dataset:\n",
    "            # per_replica_idx = strategy.run(return_const, args=(idx,))\n",
    "            for ix in strategy.experimental_local_results(idx):\n",
    "                print(len(ix))\n",
    "                index = np.append(index, ix.numpy(), axis=0)\n",
    "            print(f\"index: {index.shape}\")\n",
    "            \n",
    "            for ix in strategy.experimental_local_results(dataset):\n",
    "                print(f\"dataset-shape: {len(ix)}\")\n",
    "                \n",
    "            per_replica_embedding = strategy.run(build_embedding_in_batch, args=(dataset,))\n",
    "            # per_replica_embedding = build_embedding_in_batch(dataset)\n",
    "            # per_replica_embedding = tf.distribute.get_replica_context().all_gather(model_embedding(dataset), axis=0)\n",
    "            for embedding in strategy.experimental_local_results(per_replica_embedding):\n",
    "                embedding_img = np.append(embedding_img, embedding.numpy(), axis=0)\n",
    "            print(f\"embedding_img: {embedding_img.shape}\")\n",
    "        \n",
    "        del model_embedding\n",
    "        del model_cnn\n",
    "\n",
    "    # print(index)\n",
    "    print(index.shape)\n",
    "    print(embedding_img.shape)\n",
    "    embedding_cols = [f\"emb_{i + 1}\" for i in range(embedding_img.shape[1])]\n",
    "    embedding_df = pd.DataFrame(embedding_img, columns=embedding_cols, index=index)\n",
    "    embedding_df.reset_index(inplace=True)\n",
    "    return embedding_df\n",
    "\n",
    "# with strategy.scope():\n",
    "if 'model_img_embedding' in locals() or 'model_img_embedding' in globals():\n",
    "    del model_img_embedding\n",
    "if 'model_cnn' in locals() or 'model_cnn' in globals():\n",
    "    del model_cnn\n",
    "# model_cnn = keras.models.load_model(CNN_MODEL_FILENAME)\n",
    "# # get penultimate layer\n",
    "# model_embedding = keras.models.Model(inputs=model_cnn.input, outputs=model_cnn.layers[-3].output)\n",
    "\n",
    "dataset_seen = tf.data.Dataset.from_tensor_slices((img_seen, df_seen['image'].to_list())).batch(GLOBAL_BATCH_SIZE) \n",
    "dist_dataset_seen = strategy.experimental_distribute_dataset(dataset_seen)\n",
    "df_embeddings_img_seen = build_embedding_df(dist_dataset_seen)\n",
    "display(df_embeddings_img_seen)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensors((img_test, df_test['image'].to_list())).batch(GLOBAL_BATCH_SIZE) \n",
    "dist_dataset_test = strategy.experimental_distribute_dataset(dataset_test)\n",
    "df_embeddings_img_test = build_embedding_df(dist_dataset_test)\n",
    "display(df_embeddings_img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_complete_seen = df_seen.join(df_embeddings_img_seen.set_index('image'), on='image', how='left')\n",
    "display(df_complete_seen)\n",
    "\n",
    "df_complete_test = df_test.join(df_embeddings_img_test.set_index('image'), on='image', how='left')\n",
    "display(df_complete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Val-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "columns_cat = ['anatom_site_general', 'sex', 'anatomy_site']\n",
    "columns_to_drop = [*columns_cat, 'image', 'lesion_id', 'label']\n",
    "\n",
    "\n",
    "def usable_df(df, columns=None):\n",
    "    df_dump = df.copy()\n",
    "    y = df_dump['label']\n",
    "\n",
    "    for cat in columns_cat:\n",
    "        one_hot = pd.get_dummies(df_dump[cat], prefix=cat)\n",
    "        df_dump = df_dump.join(one_hot)\n",
    "\n",
    "    df_dump = df_dump.drop(columns_to_drop, axis=1)\n",
    "    if columns is not None:\n",
    "        missing_cols = set(columns).difference(set(df_dump.columns))\n",
    "        for miss_col in missing_cols:\n",
    "            df_dump[miss_col] = np.zeros(df_dump.shape[0])\n",
    "        df_dump = df_dump[columns]\n",
    "    return df_dump, df_dump.to_numpy(), y, df_dump.columns\n",
    "\n",
    "\n",
    "_, X_seen, y_seen, cols_usable = usable_df(df_complete_seen)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seen, y_seen, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "print(f\"Length of training data: {X_train.shape}\")\n",
    "print(f\"Length of validation data: {X_val.shape}\")\n",
    "\n",
    "_, X_test, y_test, _ = usable_df(df_complete_test, cols_usable)\n",
    "print(f\"Length of test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def display_results(model):\n",
    "    for name, X, y_true in [\n",
    "        (\"Train\", X_train, y_train),\n",
    "        (\"Validation\", X_val, y_val),\n",
    "        (\"Test\", X_test, y_test)\n",
    "    ]:\n",
    "        print(f\"=============== {name} dataset ===============\")\n",
    "        y_pred = model.predict(X)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# from the above link, some note-worthy points are below:\n",
    "# * For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n",
    "# * For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;\n",
    "# So, only \"newton-cg\", \"sag\", \"saga\", \"lbfgs\" are used and max_iter is set to 1000 to overcome ConvergenceWarning\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for solver in [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\"]:\n",
    "    print(f\"Solver: {solver}\")\n",
    "    classifier_logistic = LogisticRegression(random_state=RANDOM_STATE, solver=solver, max_iter=1000)\n",
    "    classifier_logistic.fit(X_train, y_train)\n",
    "    display_results(classifier_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "# CategoricalNB is suitable only for classification with discrete features that are categorically distributed, which is not suitable for the data available and so is skipped.\n",
    "\n",
    "for naive_bayes in [GaussianNB, MultinomialNB, ComplementNB, BernoulliNB]:\n",
    "    classifier_nb = naive_bayes()\n",
    "    classifier_nb.fit(X_train, y_train)\n",
    "    display_results(classifier_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier_knn = KNeighborsClassifier()\n",
    "classifier_knn.fit(X_train, y_train)\n",
    "display_results(classifier_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_forest = RandomForestClassifier(max_depth=32, random_state=RANDOM_STATE)\n",
    "classifier_forest.fit(X_train, y_train)\n",
    "display_results(classifier_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "classifier_adaboost = AdaBoostClassifier(random_state=RANDOM_STATE)\n",
    "classifier_adaboost.fit(X_train, y_train)\n",
    "display_results(classifier_adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier_svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "classifier_svm.fit(X_train, y_train)\n",
    "display_results(classifier_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_model_lstm():\n",
    "    clear_session()\n",
    "    reset_random_seeds()\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(128))\n",
    "    model.add(keras.layers.Dense(units=len(labels), name='output', activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model_lstm = build_model_lstm()\n",
    "model_lstm.build(input_shape=(None, 1, X_seen.shape[1]))\n",
    "model_lstm.summary()\n",
    "\n",
    "history_lstm = model_lstm.fit(np.reshape(X_seen, (X_seen.shape[0], 1, X_seen.shape[1])), y_seen, epochs=EPOCHS,\n",
    "                              batch_size=BATCH_SIZE, validation_split=TEST_SIZE).history\n",
    "plot_history(history_lstm)\n",
    "\n",
    "print(f\"Test Accuracy: {model_lstm.evaluate(X_test, y_test)[1] * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
